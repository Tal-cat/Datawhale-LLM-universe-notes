![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/c1871169-1437-4a31-a131-555222128ece)![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/c1871169-1437-4a31-a131-555222128ece)# LLM-universe     
2024.06     
教程地址：https://github.com/datawhalechina/llm-universe/tree/main      
     
第一章 大模型简介    
第一节 大语言模型LLM理论简介     
1. 目前模型：（1）国外：GPT-3.5、GPT-4、PaLM、Claude和 LLaMA 等；（2）国内：文心一言、讯飞星火、通义千问、ChatGLM、百川等。
2. GPT小知识：（1）decoder-only；（2）ChatGPT 最长支持32,000字符，知识截止2021年9月；（3）GPT-4经历了RLHF，对恶意或挑衅性查询的响应相对更为安全。    
3. LLaMA小知识：（1）对每个Transformer子层的输入进行了RMSNorm归一化/正则化；（2）：将ReLU替换为SwiGLU激活函数；（3）模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码；（4）LLaMA3的上下文长度增加了一倍，使用分组查询注意力（GQA，Grouped-Query Attention），将查询（query）分组并在组内共享键（key）和值（value）。
4. LLM的能力总结：
（1）涌现能力，即量变引起质变。包含1）上下文学习：GPT-3 首次引入，允许模型通过理解上下文并生成相应输出的方式来执行任务，而【无需额外的训练或参数更新】；2）指令遵循；3）逐步推理：采用思维链（CoT, Chain of Thought）推理策略，利用包含中间推理步骤的提示机制来解决这些任务。（2）作为基座模型支持多元应用的能力：多个应用可以只依赖于一个或少数几个大模型进行统一建设。（3）支持对话作为统一入口的能力：如Auto-GPT、微软 Jarvis 等。
5. LLM的特点：（1）巨大的规模：（2）预训练和微调：首先在大规模文本数据上进行预训练（【无标签】数据），学习通用的语言表示和知识。然后通过微调（【有标签】数据）适应特定任务；（3）上下文感知；（4）多语言支持：【个人认为这个由训练决定，可能不算特点？】；（5）多模态支持；（6）伦理和风险问题；（7）高计算资源需。

第二节 检索增强生成（RAG, Retrieval-Augmented Generation）简介    
1. LLM目前问题：信息偏差/幻觉、知识更新滞后性、内容不可追溯（LLM生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性）、领域专业知识能力欠缺、推理能力限制、应用场景适应性受限及长文本处理能力较弱（LLM在理解和生成长篇内容时受限于有限的上下文窗口，且【必须按顺序处理内容】，输入越长，速度越慢）。
2. RAG 的工作流程：分为数据处理、检索、增强和生成四个阶段。
3. RAG和微调（fine tune, 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现），见截图。
   ![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/36551e64-d46f-44d4-bc3f-fed4d7dca83a)

第三节 LangChain简介    
1. 目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。见下图：
   ![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/bc373449-1424-4625-b932-38c3b9c24343)
2. 核心组件：（1）模型输入/输出（Model I/O）：与语言模型交互的接口；（2）数据连接（Data connection）：与特定应用程序的数据进行交互的接口；（3）链（Chains）：将组件组合实现端到端应用。比如后续我们会将搭建检索问答链来完成检索问答。（4）记忆（Memory）：用于链的多次运行之间持久化应用程序状态；（5）代理（Agents）：扩展模型的推理能力。用于复杂的应用的调用序列；（6）回调（Callbacks）：扩展模型的推理能力。用于复杂的应用的调用序列。
3. 目前稳定版本v0.1.0，保持向后兼容性。
4. LangChain生态包括：（1）LangChain Community；（2）LangChain Core，就是核心库、核心组件；（3）LangChain CLI 命令行工具；（4）LangServe部署服务；LangSmith开发者平台。

第四节 开发LLM应用的整体流程     
1. 大模型开发更多是一个【工程】问题。在大模型开发中，一般不会去大幅度改动模型，而是将大模型作为一个【调用工具】，通过【Prompt Engineering、数据工程、业务逻辑分解等】手段来充分发挥大模型能力，适配应用任务。
2. 大模型开发：用【Prompt Engineering】来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用【一个通用大模型 + 若干业务Prompt】来解决任务，将传统的模型训练调优转变成了【Prompt 设计调优】。
3. 评估思路上，从实际业务需求出发【构造小批量验证集】，设计合理Prompt来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt的【BadCase】，并将Bad Case加入到验证集中，【针对性优化Prompt】，最后实现较好的泛化效果。下图为对比图：
   ![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/84531ceb-0dc8-40e1-9c04-4414ba80a27d)
4. 大模型开发的一般流程：（1）确定目标；（2）设计功能：越清晰、深入的业务逻辑理解往往也能带来更好的Prompt效果。首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；（3）搭建整体架构：目前，绝大部分大模型应用都是采用的【特定数据库+Prompt+通用大模型】的架构。推荐【基于LangChain框架】进行开发。LangChain 提供了 Chain、Tool 等架构的实现；（4）搭建数据库：个性化大模型应用需要有个性化数据库进行支撑；（5）Prompt Engineering；（6）验证迭代：在大模型开发中是极其重要的一步，【一般指通过不断发现Bad Case并针对性改进Prompt Engineering来提升系统效果、应对边界情况】。（7）前后端搭建：采用 Gradio 和 Streamlit。

第五~七节为实战，略。     

第二章 使用LLM API开发应用     
第一节 基本概念     
1. 每一次访问大模型的【输入】为一个【Prompt】，而【大模型的返回结果】为【Completion】。
2. Tempereture: 一般取值在【0~1】，当取值较低接近【0】时，预测的随机性会较低，产生【更
保守、可预测】的文本，不太可能生成意想不到或不寻常的词。当取值较高接近【1】时，预测的随机性会较高，所有词被选择的可能性更大，会产生更【有创意、多样化】的文本，更有可能生成不寻常或意想不到的词。
3. System prompt: 与User prompt相对，【整个会话过程中持久】地影响模型的回复，且相比于普通 Prompt 具有【更高的重要性】。比如GPT里面可以定制：【你是一个摸鱼高手】的system prompt，这时候用户输入【努力干活】，结果可能还是【摸鱼】。

第二节 使用LLM API 【比较实操，但目前看是用在jupyter noteook cell里写prompt，不如直接用网页写？操作部分略过，代码参见教程github链接】    

第三节 Prompt Engineering提示词工程【近期很火HOT🔥🔥🔥】     
1. 重要性：决定了其能力上限与下限。
2. 设计的【2个原则】：（1）编写清晰、具体的指令。“Adding more context helps the model understand you better.”；（2）给予模型充足思考时间。
3. 原则一：编写清晰、具体的指令：
（1）使用分隔符清晰地表示输入的不同部分【便于读取】：防止【提示词注入（Prompt Rejection），感觉翻译为提示词失效/拒绝/否认等可能更好？】→就是用户输入的文本可能与预设Prompt冲突的内容，如果不加分隔，这些输入就可能“注入”并操纵语言模型，轻则导致模型产生毫无关联的不正确的输出，严重的话可能造成应用的安全风险；
（2）寻求结构化的输出【计算机的长处】：按照某种格式组织的内容，例如JSON、HTML等；
（3）要求模型检查是否满足条件【相当于if-else语句】：如果任务包含不一定能满足的假设（条件），可以告诉模型【先检查这些假设】，如果不满足，则会指出并停止执行后续的完整流程。还可以考虑可能出现的边缘情况及模型的应对；
(4)提供少量示例【人工给模板，监督学习】:"Few-shot" prompting（少样本提示），即在要求模型执行实际任务之前，给模型提供一两个参考样例，让模型了解我们的要求和期望的输出样式。
4. 原则二：给模型时间去思考：通过Prompt引导语言模型进行深入思考。可以要求其先列出对问题的各种看法，说明推理依据，然后再得出最终结论。在Prompt中【添加逐步推理的要求】，能让语言模型【投入更多时间逻辑思维】，输出结果也将更可靠准确。
（1）指定完成任务所需的步骤。
（2）指导模型在下结论之前找出一个自己的解法。注意：若让语言模型描述一个不存在的产品,它可能会自行构造出似是而非的细节。这被称为“幻觉” (Hallucination)。

第三章 搭建知识库【学到这里感觉有些像h2o项目，之前就是这个把docker跑崩了==#】     
第一节 词向量及向量知识库     
1. Embedding嵌入：一种将非结构化数据，如单词、句子或者整个文档，转化为实数向量的技术。下图：
   <img width="526" alt="截屏2024-06-24 20 20 48" src="https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/517a0bbe-ca9b-485a-b040-8b7fa923f6d6">
2. Embedding背后思想是相似或相关的对象在嵌入空间中的距离应该很近。下图：
   <img width="496" alt="截屏2024-06-24 20 22 08" src="https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/af4a0ae5-3b5d-42cd-bfbf-358108df0c02">
3. Word Embedding词向量优势：（1）词向量比文字更适合检索：词向量中包含了原文本的语义信息，可以通过计算问题与数据库中数据的点积、余弦距离、欧几里得距离等指标，直接获取问题与数据在语义层面上的相似度；（2）词向量比其它媒介的综合信息能力更强：可以通过多种向量模型将多种数据映射成统一的向量形式。
4. 构建词向量方法：(1)使用各个公司的 Embedding API；(2)在本地使用嵌入模型将数据构建为词向量。
5. 向量数据库优势：数据以向量作为基本单位，对向量进行【存储、处理及检索】。向量数据库通过计算【与目标向量的余弦距离、点积等】获取与目标向量的相似度。当处理大量甚至海量的向量数据时，向量数据库【索引】和【查询算法】的【效率明显高于传统数据库】。
6. 主流向量数据库：（1）Chroma；（2）Weaviate；（3）Qdrant。

第二节 使用Embedding API【代码实践部分，参见原文链接即可】      
     
第三节 数据处理    
1. 本地数据库：通过前文Embedding方法将【本地文档的内容】转化为【词向量】来构建向量数据库。
2. 清洗举例：洗掉\n等。
3. 文档分割：原因：单个文档的长度往往会【超过模型支持的上下文】，导致检索得到的知识【太长超出模型的处理能力】。处理：对文档进行分割，将单个文档按长度或者按固定的规则分割成若干个chunk，然后将每个chunk转化为词向量，存储到向量数据库中。
4. 检索时会以【chunk】作为【检索的元单位】，也就是每一次检索到k个chunk作为模型可以参考来回答用户问题的知识，k为自由设定。
5. chunk_size指每个块包含的【字符或Token（如单词、句子等）】的数量。chunk_overlap指两个块之间【共享的字符数量】，用于保持上下文的连贯性，避免分割丢失上下文信息。见下图：
   <img width="568" alt="截屏2024-06-24 20 32 21" src="https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/6c01ebb8-1e3e-48d6-9af0-d0e84afd2667">

第四节 搭建并使用向量数据库【实践为主，大部分略】   
1. Chroma的相似度搜索使用的是余弦距离。
2. MMR检索：最大边际相关性 (MMR, Maximum marginal relevance)可以帮助我们在保持相关性的同时，增加内容的丰富度。核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档。    





    









