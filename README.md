# LLM-universe
2024.06     
第一章 大模型简介    
第一节 大语言模型LLM理论简介     
1. 目前模型：（1）国外：GPT-3.5、GPT-4、PaLM、Claude和 LLaMA 等；（2）国内：文心一言、讯飞星火、通义千问、ChatGLM、百川等。
2. GPT小知识：（1）decoder-only；（2）ChatGPT 最长支持32,000字符，知识截止2021年9月；（3）GPT-4经历了RLHF，对恶意或挑衅性查询的响应相对更为安全。    
3. LLaMA小知识：（1）对每个Transformer子层的输入进行了RMSNorm归一化/正则化；（2）：将ReLU替换为SwiGLU激活函数；（3）模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码；（4）LLaMA3的上下文长度增加了一倍，使用分组查询注意力（GQA，Grouped-Query Attention），将查询（query）分组并在组内共享键（key）和值（value）。
4. LLM的能力总结：
（1）涌现能力，即量变引起质变。包含1）上下文学习：GPT-3 首次引入，允许模型通过理解上下文并生成相应输出的方式来执行任务，而【无需额外的训练或参数更新】；2）指令遵循；3）逐步推理：采用思维链（CoT, Chain of Thought）推理策略，利用包含中间推理步骤的提示机制来解决这些任务。（2）作为基座模型支持多元应用的能力：多个应用可以只依赖于一个或少数几个大模型进行统一建设。（3）支持对话作为统一入口的能力：如Auto-GPT、微软 Jarvis 等。
5. LLM的特点：（1）巨大的规模：（2）预训练和微调：首先在大规模文本数据上进行预训练（【无标签】数据），学习通用的语言表示和知识。然后通过微调（【有标签】数据）适应特定任务；（3）上下文感知；（4）多语言支持：【个人认为这个由训练决定，可能不算特点？】；（5）多模态支持；（6）伦理和风险问题；（7）高计算资源需。

第二节 检索增强生成（RAG, Retrieval-Augmented Generation）简介    
1. LLM目前问题：信息偏差/幻觉、知识更新滞后性、内容不可追溯（LLM生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性）、领域专业知识能力欠缺、推理能力限制、应用场景适应性受限及长文本处理能力较弱（LLM在理解和生成长篇内容时受限于有限的上下文窗口，且【必须按顺序处理内容】，输入越长，速度越慢）。
2. RAG 的工作流程：分为数据处理、检索、增强和生成四个阶段。
3. RAG和微调（fine tune, 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现），见截图。
   ![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/36551e64-d46f-44d4-bc3f-fed4d7dca83a)

第三节 LangChain简介    
1. 目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。见下图：
   ![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/bc373449-1424-4625-b932-38c3b9c24343)
2. 核心组件：（1）模型输入/输出（Model I/O）：与语言模型交互的接口；（2）数据连接（Data connection）：与特定应用程序的数据进行交互的接口；（3）链（Chains）：将组件组合实现端到端应用。比如后续我们会将搭建检索问答链来完成检索问答。（4）记忆（Memory）：用于链的多次运行之间持久化应用程序状态；（5）代理（Agents）：扩展模型的推理能力。用于复杂的应用的调用序列；（6）回调（Callbacks）：扩展模型的推理能力。用于复杂的应用的调用序列。
3. 目前稳定版本v0.1.0，保持向后兼容性。
4. LangChain生态包括：（1）LangChain Community；（2）LangChain Core，就是核心库、核心组件；（3）LangChain CLI 命令行工具；（4）LangServe部署服务；LangSmith开发者平台。

第四节 开发LLM应用的整体流程     
1. 大模型开发更多是一个【工程】问题。在大模型开发中，一般不会去大幅度改动模型，而是将大模型作为一个【调用工具】，通过【Prompt Engineering、数据工程、业务逻辑分解等】手段来充分发挥大模型能力，适配应用任务。
2. 大模型开发：用【Prompt Engineering】来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用【一个通用大模型 + 若干业务Prompt】来解决任务，将传统的模型训练调优转变成了【Prompt 设计调优】。
3. 评估思路上，从实际业务需求出发【构造小批量验证集】，设计合理Prompt来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt的【BadCase】，并将Bad Case加入到验证集中，【针对性优化Prompt】，最后实现较好的泛化效果。下图为对比图：
   ![image](https://github.com/Tal-cat/Datawhale-LLM-universe-notes/assets/60603537/84531ceb-0dc8-40e1-9c04-4414ba80a27d)
4. 大模型开发的一般流程：（1）确定目标；（2）设计功能：越清晰、深入的业务逻辑理解往往也能带来更好的Prompt效果。首先要确定应用的核心功能，然后延展设计核心功能的上下游功能；（3）搭建整体架构：目前，绝大部分大模型应用都是采用的【特定数据库+Prompt+通用大模型】的架构。推荐【基于LangChain框架】进行开发。LangChain 提供了 Chain、Tool 等架构的实现；（4）搭建数据库：个性化大模型应用需要有个性化数据库进行支撑；（5）Prompt Engineering；（6）验证迭代：在大模型开发中是极其重要的一步，【一般指通过不断发现Bad Case并针对性改进Prompt Engineering来提升系统效果、应对边界情况】。（7）前后端搭建：采用 Gradio 和 Streamlit。

第五~七节为实战，略。   
    









